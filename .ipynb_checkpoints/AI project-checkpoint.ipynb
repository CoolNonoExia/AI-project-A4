{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "print('Hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from cmath import rect, phase\n",
    "from math import radians, degrees\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../csv/general_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-979ac50807b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Import CSV files to variables of type pandas.DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgeneral_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../csv/general_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0memployee_survey_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../csv/employee_survey_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmanager_survey_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../csv/manager_survey_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/OMEN/APPS/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/OMEN/APPS/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/OMEN/APPS/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/OMEN/APPS/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/OMEN/APPS/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../csv/general_data.csv'"
     ]
    }
   ],
   "source": [
    "# Set unlimited columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "path = '../'\n",
    "print(\"Only directories:\")\n",
    "\n",
    "print([ name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name)) ])\n",
    "# Import CSV files to variables of type pandas.DataFrame\n",
    "general_data = pd.read_csv('../csv/general_data.csv')\n",
    "employee_survey_data = pd.read_csv('../csv/employee_survey_data.csv')\n",
    "manager_survey_data = pd.read_csv('../csv/manager_survey_data.csv')\n",
    "in_time = pd.read_csv('../csv/in_time.csv')\n",
    "out_time = pd.read_csv('../csv/out_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement de general_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va vérifier la répartition des valeurs dans notre dataset general_data sur certaines colonnes. Si tous les employés ont la même valeur pour une même colonne alors cette colonne est inutile : autant l'éliminer.\n",
    "\n",
    "Pour cela on utilisera la fonction value_counts : dans l'exemple ci-dessous on voit les différents âges et combien d'employés par valeur d'âge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " general_data[\"Age\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'extrait du DataFrame plus haut, on repère seulement 3 colonnes qui ont la même valeur pour les 5 employés de l'extrait, on vérifie si cela se généralise dans tout le DataFrame ou si c'était un hasard avec les 5 premiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(general_data[\"EmployeeCount\"].value_counts())\n",
    "print(general_data[\"Over18\"].value_counts())\n",
    "print(general_data[\"StandardHours\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes EmployeeCount, Over18 et StandardHours sont les mêmes pour tous les employés, elles sont donc à éliminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_data.drop(columns=['EmployeeCount', 'Over18','StandardHours'], inplace=True)\n",
    "general_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va transformer certaiens données présentes en  format numérique sous formes de caractères litteraux pour plus de compréhension. Ici Education peut être interessant à changer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_data[\"Education\"] = general_data['Education'].replace({ 1 : 'Below College', 2: 'College',3: 'Bachelor',4: 'Master',5 : 'Doctor'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que certaines colonnes : \"NumCompaniesWorked\" et \"TotalWorkingYears\" sont parfois non renseignées, on laisse les valeurs vides pour le moment mais il faudra peut-être les gérer si le besoin se présente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des questionnaires (employee_survey_data  et manager_survey_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_survey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On va de la même façon que le premier fichier modifier nos colonnes numériques importantes en litteraux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee_survey_data['WorkLifeBalance'] = employee_survey_data['WorkLifeBalance'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\n",
    "# employee_survey_data['EnvironmentSatisfaction'] = employee_survey_data['EnvironmentSatisfaction'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\n",
    "# employee_survey_data['JobSatisfaction'] = employee_survey_data['JobSatisfaction'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\n",
    "\n",
    "#employee_survey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le fichier des questionnaires managers, on fait pareil pour la note de l'implication et de la qualité de travail au cours de l'année"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_survey_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manager_survey_data['JobInvolvement'] = manager_survey_data['JobInvolvement'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\n",
    "#manager_survey_data['PerformanceRating'] = manager_survey_data['PerformanceRating'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\n",
    "\n",
    "#manager_survey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le questionnaires employés, ceux ci ne sont pas obligés de répondre, il faudra donc traiter les valeurs non renseignées pour notre apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des données sur les horaires de travail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va traiter deux fichiers : les horaires d'éntrées et de sorties pour chaque employé et pour chaque jour de l'anée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset in_time / out_time :\n",
    "#in_time = pd.read_csv('../csv/in_time.csv')\n",
    "#out_time = pd.read_csv('../csv/out_time.csv')\n",
    "\n",
    "in_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher les colonnes ou tous les employées n'ont pas été présents (NaN) : on supprimera celles ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = in_time.columns[in_time.nunique() <= 1]\n",
    "\n",
    "print(holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dataframe répresente toute les dates pour lesquelles les employées n'étaient pas présents, on va les supprimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time = in_time.drop(columns = holidays)\n",
    "\n",
    "in_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on va remplacer les valeurs non définies par 0 et convertir l'ensemble au format datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time = in_time.replace(np.nan, 0)\n",
    "in_time.iloc[:, 1:] = in_time.iloc[:, 1:].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "in_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait les mêmes manipulations avec le dataframe des horraires de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_time = out_time.drop(columns = holidays)\n",
    "\n",
    "out_time = out_time.replace(np.nan,0)\n",
    "out_time.iloc[:, 1:] = out_time.iloc[:, 1:].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre phase d'apprentissage, il est important pour nous de mettre en place des moyennes.\n",
    "\n",
    "On va commencer par faire une moyenne du temps de travail pour chaque employé.\n",
    "\n",
    "Pour mieux y voir, on va mélanger nos deux dataframes précedent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_time = in_time.append(out_time)\n",
    "\n",
    "in_out_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_number = (int) (in_out_time.shape[0] / 2)\n",
    "\n",
    "# On fait la différence entre l'heure de départ et l'heure d'arrivée\n",
    "in_out_time = in_out_time.diff(periods=employees_number)\n",
    "# La seconde moitié du tableau (anciennement out_time) est désormais aux valeurs de durée\n",
    "\n",
    "# On supprime la première moitié du tableau (in_time) puis on reset les index\n",
    "in_out_time = in_out_time.iloc[employees_number:]\n",
    "in_out_time.reset_index(inplace=True)\n",
    "\n",
    "in_out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos deux premières colonnes sont inutiles, on va les supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_time.drop(columns=['index', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "in_out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée une colonne contenant la moyenne de ces temps de travail pour chaque employé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_time['AverageWorkTimeDelta'] = in_out_time.mean(axis=1)\n",
    "\n",
    "in_out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut transformer la moyenne d'heure en heures pour plus de clarté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_time['AverageWorkTime'] = in_out_time['AverageWorkTimeDelta'] / np.timedelta64(1, 'h')\n",
    "in_out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute le nombre d'absences de chaque employé (hors absences généralisées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_time['NumberOfAbsences'] = (in_out_time.drop(['AverageWorkTimeDelta', 'AverageWorkTime'], 1) == np.timedelta64(0)).sum(axis=1)\n",
    "\n",
    "in_out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant faire une moyenne des heures d'arrivée et de départ de chaque employé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_angle(deg):\n",
    "    return degrees(phase(sum(rect(1, radians(d)) for d in deg)/len(deg)))\n",
    " \n",
    "def mean_time(row):\n",
    "    t = ([time.hour, time.minute, time.second] for time in row)\n",
    "    seconds = ((float(s) + int(m) * 60 + int(h) * 3600) \n",
    "               for h, m, s in t)\n",
    "    \n",
    "    # Remove NaN / 0\n",
    "    seconds = [i for i in seconds if i > 0]\n",
    "    \n",
    "    day = 24 * 60 * 60\n",
    "    to_angles = [s * 360. / day for s in seconds]\n",
    "    mean_as_angle = mean_angle(to_angles)\n",
    "    mean_seconds = mean_as_angle * day / 360.\n",
    "    if mean_seconds < 0:\n",
    "        mean_seconds += day\n",
    "    h, m = divmod(mean_seconds, 3600)\n",
    "    m, s = divmod(m, 60)\n",
    "    return '%02i:%02i:%02i' % (h, m, s)\n",
    "\n",
    "in_out_time['AverageInTime'] = in_time.drop(['Unnamed: 0'], 1).apply(mean_time, axis=1)\n",
    "in_out_time['AverageOutTime'] = out_time.drop(['Unnamed: 0'], 1).apply(mean_time, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va désormais drop les colonnes de chaque jour qui ne nous sont plus utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmployeeID commence à 1 au lieu de 0\n",
    "in_out_time.index += 1\n",
    "\n",
    "in_out_time.reset_index(inplace=True)\n",
    "in_out_time.rename(columns={'index': 'EmployeeID'}, inplace=True)\n",
    "in_out_time.drop(in_out_time.columns.difference(['EmployeeID', 'AverageWorkTime', 'NumberOfAbsences', 'AverageInTime', 'AverageOutTime']), 1, inplace=True)\n",
    "\n",
    "in_out_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion en un seul DataFrame\n",
    "On s'occupe maintenant de fusionner toutes les données en un seul DataFrame, en faisant correspondre les EmployeeID.\n",
    "Ce DataFrame constituera notre le jeu de donnée de base du projet à partir d'ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(employee_survey_data, manager_survey_data, how='inner', on='EmployeeID')\n",
    "df = pd.merge(df, general_data, how='inner', on='EmployeeID')\n",
    "df = pd.merge(df, in_out_time, how='inner', on='EmployeeID')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- Essais statistiques ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en évidence de la variable clé Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df['Attrition'].value_counts(normalize = True),2))\n",
    "sns.countplot(x='Attrition',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['Age','MonthlyIncome','DistanceFromHome','Attrition']],hue = 'Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fréquence de voyages (en %): \\n')\n",
    "print(round(df['BusinessTravel'].value_counts(normalize = True)*100,2))\n",
    "print(\"\\n Taux d'attrition par type de voyages \\n\")\n",
    "print(round(df['BusinessTravel'][df['Attrition'] == 'Yes'].value_counts()/df['BusinessTravel'].value_counts()*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombres d'émployés par département (en %): \\n\")\n",
    "print(round(df['Department'].value_counts(normalize = True)*100,2))  \n",
    "print(\"\\n Taux d'attrition par département \\n\")\n",
    "print(round(df['Department'][df['Attrition'] == 'Yes'].value_counts()/df['Department'].value_counts()*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre d'émployées par niveau d'éducation (en %): \\n\")\n",
    "print(round(df['Education'].value_counts(normalize = True)*100,2).sort_index())\n",
    "print(\"\\n Taux d'attrition par niveau d'éducation : \\n\")\n",
    "print(round(df['Education'][df['Attrition'] == 'Yes'].value_counts()/df['Education'].value_counts()*100,2).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut chercher d'autre types de corrélation comme celle entre l'attarition et le statut marital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Taux d'attrition par statut marital: \\n\")\n",
    "print(round(df['MaritalStatus'][df['Attrition'] == 'Yes'].value_counts()/df['MaritalStatus'].value_counts()*100,2))\n",
    "(df['MaritalStatus'][df['Attrition'] == 'Yes'].value_counts()/df['MaritalStatus'].value_counts()*100).plot.bar(color = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également chercher le nombre d'heures moyen et le taux d'attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['AverageWorkTime', 'JobSatisfaction', 'Attrition']],hue = 'Attrition', height = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines de nos colonnes ne sont pas entièrement remplies : on va les remplir avec les valeurs médianes adéquates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnvironmentSatisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='EnvironmentSatisfaction', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EnvironmentSatisfaction'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EnvironmentSatisfaction\"].fillna(df[\"EnvironmentSatisfaction\"].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JobSatisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='JobSatisfaction', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JobSatisfaction'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"JobSatisfaction\"].fillna(df[\"JobSatisfaction\"].median(), inplace=True)\n",
    "df['JobSatisfaction'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WorkLifeBalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WorkLifeBalance'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='WorkLifeBalance',data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"WorkLifeBalance\"].fillna(df[\"WorkLifeBalance\"].median(), inplace=True)\n",
    "df['WorkLifeBalance'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumCompaniesWorked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NumCompaniesWorked'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='NumCompaniesWorked',data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='NumCompaniesWorked',data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NumCompaniesWorked'].fillna(df.groupby(['TotalWorkingYears'])['NumCompaniesWorked'].transform('median'), inplace=True)\n",
    "df['NumCompaniesWorked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TotalWorkingYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalWorkingYears'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ax = sns.distplot(df['TotalWorkingYears'], hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "ax.set_ylabel('# of Employees')\n",
    "ax.set_xlabel('TotalWorkingYears');\n",
    "\n",
    "#Expliquer le schéma. kde = gaussian kernel density estimate => courbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='TotalWorkingYears',data=df);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalWorkingYears'].fillna(df.groupby(['Age'])['TotalWorkingYears'].transform('median'), inplace=True)\n",
    "df['TotalWorkingYears'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalWorkingYears'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On à désormais une harmonisation de donéés grâce à notre traitement de valeurs nulles ou non renseignées."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IMPLEMENTATION DE LA FORET ALEATOIRE</h2>\n",
    "\n",
    "<h3>Presentation</h3>\n",
    "\n",
    "<p>\n",
    "    L’algorithme des « forêts aléatoires » (ou Random Forest parfois aussi traduit par forêt d’arbres décisionnels) est un algorithme de classification qui réduit la variance des prévisions d’un arbre de décision seul, améliorant ainsi leurs performances. Pour cela, il combine de nombreux arbres de décisions dans une approche de type bagging.\n",
    "    </p>\n",
    "    <p>\n",
    "    Dans sa formule la plus classique, il effectue un apprentissage en parallèle sur de multiples arbres de décision construits aléatoirement et entraînés sur des sous-ensembles de données différents. Le nombre idéal d’arbres, qui peut aller jusqu’à plusieurs centaines voire plus, est un paramètre important : il est très variable et dépend du problème. Concrètement, chaque arbre de la forêt aléatoire est entrainé sur un sous ensemble aléatoire de données selon le principe du bagging, avec un sous ensemble aléatoire de features (caractéristiques variables des données) selon le principe des « projections aléatoires ». Les prédictions sont ensuite moyennées lorsque les données sont quantitatives ou utilisés pour un vote pour des données qualitatives, dans le cas des arbres de classification. L’algorithme des forêts aléatoires est connu pour être un des classifieurs les plus efficaces « out-of-the-box » (c’est-à-dire nécessitant peu de prétraitement des données). Il a été utilisé dans de nombreuses applications, y compris grand public, comme pour la classification d’images de la caméra de console de jeu Kinect* dans le but d’identifier des positions du corps.\n",
    "    </p>\n",
    "<img src=\"https://static.wixstatic.com/media/50480c_09887082457042d1a9a68ba1efd3ab41~mv2.png/v1/fit/w_592%2Ch_444%2Cal_c/file.png\"/>\n",
    "\n",
    "\n",
    "<p>\n",
    "    Un random forest est constitué d'un ensemble d'arbres de décision indépendants. \n",
    "\n",
    "Chaque arbre dispose d'une vision parcellaire du problème du fait d'un double tirage aléatoire :\n",
    "\n",
    "<li>un tirage aléatoire avec remplacement sur les observations (les lignes de votre base de données). Ce processus s'appelle le tree bagging,</li>\n",
    "<li>un tirage aléatoire sur les variables (les colonnes de votre base de données). Ce processus s'appelle le feature sampling.</li>\n",
    "A la fin, tous ces arbres de décisions indépendants sont assemblés. La prédiction faite par le random forest pour des données inconnues est alors la moyenne (ou le vote, dans le cas d'un problème de classification) de tous les arbres.\n",
    "    </p>\n",
    "    \n",
    "<h2> Implementation</h2>\n",
    "<p>On va d'abbord numeriser notre étiquette Attrition =></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a dummy variable for some of the categorical variables and dropping the first one.\n",
    "dummy1 = pd.get_dummies(df[['JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction',\n",
    "                                 'JobSatisfaction', 'WorkLifeBalance','BusinessTravel', 'Department',\n",
    "                                 'Education','EducationField', 'Gender', 'JobLevel', 'JobRole',\n",
    "                                 'MaritalStatus']], drop_first=True)\n",
    "\n",
    "# Adding the results to the master dataframe\n",
    "df = pd.concat([df, dummy1], axis=1)\n",
    "df = df.drop(['JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction',\n",
    "                                 'JobSatisfaction', 'WorkLifeBalance','BusinessTravel', 'Department',\n",
    "                                 'Education','EducationField', 'Gender', 'JobLevel', 'JobRole',\n",
    "                                 'MaritalStatus'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attrition'] = df['Attrition'].replace({'Yes': 1, \"No\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['AverageOutTime', 'AverageInTime', 'Attrition' ], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Attrition']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n",
    "           'DistanceFromHome','Age','AverageWorkTime', 'NumberOfAbsences']] = scaler.fit_transform(X_train[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n",
    "           'DistanceFromHome', 'Age','AverageWorkTime', 'NumberOfAbsences']])\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n",
    "           'DistanceFromHome','Age','AverageWorkTime', 'NumberOfAbsences']] = scaler.fit_transform(X_test[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n",
    "           'DistanceFromHome', 'Age','AverageWorkTime', 'NumberOfAbsences']])\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "df.corr()['Attrition'].sort_values(ascending = False).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Arbres utilises : ', model.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = model.predict(X_train)\n",
    "predict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainaccuracy = accuracy_score(y_train,predict_train)\n",
    "print('accuracy_score on train dataset : ', trainaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
